# Simple RAG (Retrieval-Augmented Generation) Project

This project demonstrates a **simple implementation of Retrieval-Augmented Generation (RAG)** using Python and popular libraries such as `langchain`, `Chroma`.

## ğŸ” What is RAG?

**Retrieval-Augmented Generation (RAG)** is a method that enhances the performance of language models by retrieving relevant context from a knowledge base or document store and feeding it into a generative model to provide accurate and contextually aware answers.

## ğŸ§  How It Works

1. **Document Ingestion**: Load and chunk your documents.
2. **Embedding Generation**: Convert chunks into vector representations using pre-trained embeddings.
3. **Vector Store (Chroma)**: Store the embeddings in a Chroma index for fast retrieval.
4. **Query Handling**: When a user asks a question, retrieve the most relevant chunks using similarity search.
5. **Answer Generation**: Feed the retrieved context and the question into a generative language model like OpenAI or HuggingFace Transformers to get an answer.

---

## ğŸš€ Run the Project

You can test and explore this project directly on **Google Colab** by clicking the link below:

ğŸ‘‰ [Open in Colab](https://colab.research.google.com/drive/1OF8K0vh2FfPTWmic1ru02eU0S1Xvq0JB?usp=sharing)

---

## ğŸ“¦ Dependencies

Make sure to install the required dependencies. On Colab, these are pre-installed or included in the notebook setup.

Make saure to use your own env variables.
```bash
os.environ["OPENAI_API_KEY"]="XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
os.environ["LANGCHAIN_TRACING_V2"]="true"
os.environ["LANGCHAIN_PROJECT"]="langchain-basics"
os.environ["LANGCHAIN_API_KEY"]="XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
```
---

## ğŸ› ï¸ Steps to Reproduce

1. **Clone the repository or open the Colab link**.
2. **Upload your documents** (can be PDF, text files, etc.).
3. Follow the steps in the notebook:
   - âœ… Load and preprocess the document.
   - ğŸ§± Split the document into smaller, manageable chunks.
   - ğŸ“ Generate embeddings using OpenAI Embeddings or another embedding model.
   - ğŸ§  Store the embeddings in a Chroma vector store.
   - ğŸ” Use similarity search to retrieve the most relevant chunks based on user queries.
   - âœ¨ Pass the relevant context and user question into a generative language model to generate an accurate response.

---

## ğŸ“š Use Cases

- Chatbots with custom knowledge
- Search-based Q&A systems
- Personalized AI assistants

---

## ğŸ“¬ Feedback

Feel free to fork the notebook and modify it for your own use case. If you have any feedback or questions, feel free to raise an issue or reach out!
